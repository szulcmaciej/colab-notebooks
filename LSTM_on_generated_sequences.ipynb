{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM on generated sequences",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szulcmaciej/colab-notebooks/blob/main/LSTM_on_generated_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rIuIxiRDtE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "6075d7e2-d796-4ee1-c48a-85284d0e59ac"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from tensorflow import keras\n",
        "import random\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_XvqgYQEEr-"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pasa2O0Qr5p"
      },
      "source": [
        "NUM_SEQUENCES = 100000\n",
        "SEQUENCE_MIN_LENGTH = 3\n",
        "SEQUENCE_MAX_LENGTH = 30\n",
        "MUTATION_PROB = 0.3\n",
        "ACTIONS = list(range(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl4AOZuwKY7j"
      },
      "source": [
        "sequences = []\n",
        "np.random.seed(12345)\n",
        "for i in range(NUM_SEQUENCES):\n",
        "  seq = [0]\n",
        "  np.random.seed(i*123)\n",
        "  seq_length = np.random.randint(SEQUENCE_MIN_LENGTH, SEQUENCE_MAX_LENGTH + 1)\n",
        "  # seq_length = 20\n",
        "\n",
        "  # linear\n",
        "  seq = list(range(seq_length))\n",
        "\n",
        "  # sinus\n",
        "  # seq = list(map(lambda idx, x: x + np.floor(3 * np.sin(idx)), seq))\n",
        "  seq = [min(SEQUENCE_MAX_LENGTH, max(0, x + np.floor(3 * np.sin(0.5 * idx + i)))) for idx, x in enumerate(seq)]\n",
        "\n",
        "  # randomness\n",
        "  # for j in range(len(seq) - 1):\n",
        "  #     np.random.seed(j*i*12+123*i+10*j)\n",
        "  #     if np.random.rand() < MUTATION_PROB:\n",
        "  #       seq[j], seq[j+1] = seq[j+1], seq[j]\n",
        "    \n",
        "  sequences.append(np.array(seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDkHQRNNPqF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "79aea658-fc02-4a99-a533-341d7d724645"
      },
      "source": [
        "sequences[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.,  2.,  4.,  5.,  6.,  6.,  6.,  5.,  5.,  6.,  7.,  8., 11.,\n",
              "        13., 15.]),\n",
              " array([ 2.,  3.,  4.,  4.,  4.,  3.,  3.,  4.,  5.,  6.,  9., 11., 13.,\n",
              "        15., 16., 17.]),\n",
              " array([2., 2., 2., 1., 1., 2., 3.]),\n",
              " array([ 0.,  0.,  0.,  0.,  1.,  2.,  5.,  7.,  9., 11., 12., 13., 13.,\n",
              "        12., 12., 12., 13.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 424
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHHRApWcYtUu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a1832ccc-193b-4284-a73d-9c0b00e487c5"
      },
      "source": [
        "plt.hist(list(map(len, sequences)), bins=28);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSbELXAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJBnQcWpxxVr"
      },
      "source": [
        "### padding sequences\n",
        "sequences_from_1 = list(map(lambda x: x + 1, sequences))\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences_from_1, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsi0q-BF7rrl"
      },
      "source": [
        "# data_gen = TimeseriesGenerator(X, X,\n",
        "#                                length=10, sampling_rate=2, \n",
        "#                                batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGkM96irzpup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12c1c2ff-eb00-4afa-84a0-60a3f8bde76e"
      },
      "source": [
        "padded_sequences.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 432
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ3dMymo7Vyl"
      },
      "source": [
        "X = np.array(padded_sequences)[:,:-1]\n",
        "# X = np.array(padded_sequences).reshape((*padded_sequences.shape, 1))[:,:-1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyfsmqdd9Aej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6773278c-6ee3-4488-87fc-1cffcb142a53"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ccf_4j9CFk"
      },
      "source": [
        "y = np.array(padded_sequences)[:,1:]\n",
        "# y = np.array(padded_sequences).reshape((*padded_sequences.shape, 1))[:,1:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLhVyaEr-wRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11f23320-cd57-47b4-e894-af4344f84884"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 436
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eDtN48O--uR"
      },
      "source": [
        "# X[:5, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw0vaI6A_Acv"
      },
      "source": [
        "# y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Upq9CAbp_3r"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TgyGAFwqB7v"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, TimeDistributed, LSTM, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTCK3K4ftBBi"
      },
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# model.add(LSTM(32, return_sequences=True, input_shape=(None, 1)))\n",
        "# model.add(LSTM(8, return_sequences=True))\n",
        "# model.add(TimeDistributed(Dense(20, activation='softmax')))\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer='adam')\n",
        "\n",
        "# def train_generator():\n",
        "#     while True:\n",
        "#         sequence_length = np.random.randint(10, 100)\n",
        "#         x_train = np.random.random((10, sequence_length, 1))\n",
        "#         # y_train will depend on past 5 timesteps of x\n",
        "#         y_train = x_train[:, :, 0].copy()\n",
        "#         for i in range(1, 5):\n",
        "#             y_train[:, i:] += x_train[:, :-i, 0]\n",
        "#         y_train = to_categorical(y_train > 2.5, num_classes=20)\n",
        "#         yield x_train, y_train\n",
        "\n",
        "# model.fit_generator(train_generator(), steps_per_epoch=30, epochs=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5zV6IVZARbU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cb45a3b-2733-443e-ef23-98775e33cd2a"
      },
      "source": [
        "y_categorical = to_categorical(y)\n",
        "# y_categorical = to_categorical(y, SEQUENCE_MAX_LENGTH+1)\n",
        "y_categorical.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 29, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 441
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CaTkjas_UdV"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=32, output_dim=16, mask_zero=True))\n",
        "model.add(LSTM(32, return_sequences=True, input_shape=(None, 1)))\n",
        "# model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(8, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(y_categorical.shape[2], activation='softmax')))\n",
        "# model.add(Dense(20, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnUYdysqAAR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4799c7d0-ced2-4c6d-b44e-bc3e65b5719d"
      },
      "source": [
        "# %%timeit -n 1 -r 1\n",
        "history = model.fit(X, y_categorical, batch_size=512, epochs=100, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 70000 samples, validate on 30000 samples\n",
            "Epoch 1/100\n",
            "70000/70000 [==============================] - 25s 356us/step - loss: 3.1205 - acc: 0.0831 - val_loss: 2.6982 - val_acc: 0.1566\n",
            "Epoch 2/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 2.4497 - acc: 0.2188 - val_loss: 2.2530 - val_acc: 0.2760\n",
            "Epoch 3/100\n",
            "70000/70000 [==============================] - 17s 250us/step - loss: 2.1280 - acc: 0.2901 - val_loss: 2.0241 - val_acc: 0.3143\n",
            "Epoch 4/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 1.9326 - acc: 0.3508 - val_loss: 1.8462 - val_acc: 0.3923\n",
            "Epoch 5/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 1.7782 - acc: 0.4261 - val_loss: 1.7173 - val_acc: 0.4671\n",
            "Epoch 6/100\n",
            "70000/70000 [==============================] - 18s 251us/step - loss: 1.6683 - acc: 0.4863 - val_loss: 1.6225 - val_acc: 0.5068\n",
            "Epoch 7/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 1.5839 - acc: 0.5177 - val_loss: 1.5455 - val_acc: 0.5330\n",
            "Epoch 8/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 1.5104 - acc: 0.5396 - val_loss: 1.4758 - val_acc: 0.5581\n",
            "Epoch 9/100\n",
            "70000/70000 [==============================] - 18s 254us/step - loss: 1.4427 - acc: 0.5758 - val_loss: 1.4082 - val_acc: 0.6047\n",
            "Epoch 10/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 1.3735 - acc: 0.6262 - val_loss: 1.3381 - val_acc: 0.6494\n",
            "Epoch 11/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 1.3097 - acc: 0.6560 - val_loss: 1.2825 - val_acc: 0.6702\n",
            "Epoch 12/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 1.2577 - acc: 0.6700 - val_loss: 1.2306 - val_acc: 0.6822\n",
            "Epoch 13/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 1.2065 - acc: 0.6863 - val_loss: 1.1818 - val_acc: 0.6966\n",
            "Epoch 14/100\n",
            "70000/70000 [==============================] - 17s 250us/step - loss: 1.1594 - acc: 0.7034 - val_loss: 1.1425 - val_acc: 0.7063\n",
            "Epoch 15/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 1.1192 - acc: 0.7186 - val_loss: 1.0999 - val_acc: 0.7300\n",
            "Epoch 16/100\n",
            "70000/70000 [==============================] - 18s 250us/step - loss: 1.0847 - acc: 0.7298 - val_loss: 1.0688 - val_acc: 0.7246\n",
            "Epoch 17/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 1.0528 - acc: 0.7353 - val_loss: 1.0356 - val_acc: 0.7356\n",
            "Epoch 18/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 1.0212 - acc: 0.7425 - val_loss: 1.0065 - val_acc: 0.7445\n",
            "Epoch 19/100\n",
            "70000/70000 [==============================] - 18s 250us/step - loss: 0.9925 - acc: 0.7484 - val_loss: 0.9782 - val_acc: 0.7534\n",
            "Epoch 20/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.9662 - acc: 0.7535 - val_loss: 0.9590 - val_acc: 0.7396\n",
            "Epoch 21/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.9407 - acc: 0.7597 - val_loss: 0.9296 - val_acc: 0.7638\n",
            "Epoch 22/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.9198 - acc: 0.7635 - val_loss: 0.9135 - val_acc: 0.7596\n",
            "Epoch 23/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 0.9004 - acc: 0.7666 - val_loss: 0.8911 - val_acc: 0.7686\n",
            "Epoch 24/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.8817 - acc: 0.7691 - val_loss: 0.8771 - val_acc: 0.7670\n",
            "Epoch 25/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.8654 - acc: 0.7704 - val_loss: 0.8553 - val_acc: 0.7737\n",
            "Epoch 26/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.8487 - acc: 0.7743 - val_loss: 0.8429 - val_acc: 0.7742\n",
            "Epoch 27/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.8347 - acc: 0.7758 - val_loss: 0.8256 - val_acc: 0.7785\n",
            "Epoch 28/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.8192 - acc: 0.7793 - val_loss: 0.8112 - val_acc: 0.7827\n",
            "Epoch 29/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.8051 - acc: 0.7829 - val_loss: 0.7974 - val_acc: 0.7856\n",
            "Epoch 30/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.7932 - acc: 0.7832 - val_loss: 0.7847 - val_acc: 0.7866\n",
            "Epoch 31/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.7793 - acc: 0.7868 - val_loss: 0.7757 - val_acc: 0.7837\n",
            "Epoch 32/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.7669 - acc: 0.7895 - val_loss: 0.7611 - val_acc: 0.7899\n",
            "Epoch 33/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.7554 - acc: 0.7918 - val_loss: 0.7516 - val_acc: 0.7930\n",
            "Epoch 34/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 0.7455 - acc: 0.7923 - val_loss: 0.7399 - val_acc: 0.7919\n",
            "Epoch 35/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.7351 - acc: 0.7938 - val_loss: 0.7291 - val_acc: 0.7961\n",
            "Epoch 36/100\n",
            "70000/70000 [==============================] - 18s 251us/step - loss: 0.7252 - acc: 0.7953 - val_loss: 0.7218 - val_acc: 0.7963\n",
            "Epoch 37/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.7168 - acc: 0.7963 - val_loss: 0.7121 - val_acc: 0.7930\n",
            "Epoch 38/100\n",
            "70000/70000 [==============================] - 18s 251us/step - loss: 0.7088 - acc: 0.7975 - val_loss: 0.7036 - val_acc: 0.7972\n",
            "Epoch 39/100\n",
            "70000/70000 [==============================] - 18s 250us/step - loss: 0.7002 - acc: 0.7992 - val_loss: 0.6954 - val_acc: 0.7995\n",
            "Epoch 40/100\n",
            "70000/70000 [==============================] - 17s 250us/step - loss: 0.6927 - acc: 0.8010 - val_loss: 0.6916 - val_acc: 0.7993\n",
            "Epoch 41/100\n",
            "70000/70000 [==============================] - 18s 250us/step - loss: 0.6852 - acc: 0.8027 - val_loss: 0.6821 - val_acc: 0.8036\n",
            "Epoch 42/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 0.6779 - acc: 0.8036 - val_loss: 0.6738 - val_acc: 0.8059\n",
            "Epoch 43/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.6715 - acc: 0.8050 - val_loss: 0.6693 - val_acc: 0.8033\n",
            "Epoch 44/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.6658 - acc: 0.8046 - val_loss: 0.6654 - val_acc: 0.8022\n",
            "Epoch 45/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.6570 - acc: 0.8067 - val_loss: 0.6524 - val_acc: 0.8090\n",
            "Epoch 46/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.6504 - acc: 0.8076 - val_loss: 0.6471 - val_acc: 0.8077\n",
            "Epoch 47/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.6445 - acc: 0.8090 - val_loss: 0.6401 - val_acc: 0.8101\n",
            "Epoch 48/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.6401 - acc: 0.8108 - val_loss: 0.6345 - val_acc: 0.8115\n",
            "Epoch 49/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.6332 - acc: 0.8148 - val_loss: 0.6299 - val_acc: 0.8161\n",
            "Epoch 50/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.6274 - acc: 0.8172 - val_loss: 0.6276 - val_acc: 0.8176\n",
            "Epoch 51/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.6232 - acc: 0.8176 - val_loss: 0.6196 - val_acc: 0.8166\n",
            "Epoch 52/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.6177 - acc: 0.8189 - val_loss: 0.6155 - val_acc: 0.8181\n",
            "Epoch 53/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.6138 - acc: 0.8188 - val_loss: 0.6123 - val_acc: 0.8198\n",
            "Epoch 54/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.6111 - acc: 0.8184 - val_loss: 0.6090 - val_acc: 0.8189\n",
            "Epoch 55/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.6048 - acc: 0.8203 - val_loss: 0.6027 - val_acc: 0.8199\n",
            "Epoch 56/100\n",
            "70000/70000 [==============================] - 18s 253us/step - loss: 0.6009 - acc: 0.8207 - val_loss: 0.5980 - val_acc: 0.8209\n",
            "Epoch 57/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.5975 - acc: 0.8209 - val_loss: 0.5957 - val_acc: 0.8216\n",
            "Epoch 58/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5932 - acc: 0.8216 - val_loss: 0.5938 - val_acc: 0.8205\n",
            "Epoch 59/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5890 - acc: 0.8221 - val_loss: 0.5881 - val_acc: 0.8219\n",
            "Epoch 60/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 0.5857 - acc: 0.8224 - val_loss: 0.5832 - val_acc: 0.8223\n",
            "Epoch 61/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5831 - acc: 0.8219 - val_loss: 0.5813 - val_acc: 0.8231\n",
            "Epoch 62/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.5815 - acc: 0.8213 - val_loss: 0.5792 - val_acc: 0.8210\n",
            "Epoch 63/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5764 - acc: 0.8228 - val_loss: 0.5808 - val_acc: 0.8194\n",
            "Epoch 64/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5732 - acc: 0.8232 - val_loss: 0.5723 - val_acc: 0.8232\n",
            "Epoch 65/100\n",
            "70000/70000 [==============================] - 17s 250us/step - loss: 0.5707 - acc: 0.8232 - val_loss: 0.5693 - val_acc: 0.8242\n",
            "Epoch 66/100\n",
            "70000/70000 [==============================] - 17s 242us/step - loss: 0.5689 - acc: 0.8229 - val_loss: 0.5661 - val_acc: 0.8231\n",
            "Epoch 67/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.5656 - acc: 0.8233 - val_loss: 0.5647 - val_acc: 0.8239\n",
            "Epoch 68/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5630 - acc: 0.8235 - val_loss: 0.5618 - val_acc: 0.8248\n",
            "Epoch 69/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5615 - acc: 0.8233 - val_loss: 0.5601 - val_acc: 0.8250\n",
            "Epoch 70/100\n",
            "70000/70000 [==============================] - 18s 251us/step - loss: 0.5596 - acc: 0.8234 - val_loss: 0.5630 - val_acc: 0.8202\n",
            "Epoch 71/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5561 - acc: 0.8240 - val_loss: 0.5565 - val_acc: 0.8236\n",
            "Epoch 72/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5539 - acc: 0.8244 - val_loss: 0.5536 - val_acc: 0.8250\n",
            "Epoch 73/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5531 - acc: 0.8238 - val_loss: 0.5510 - val_acc: 0.8243\n",
            "Epoch 74/100\n",
            "70000/70000 [==============================] - 17s 249us/step - loss: 0.5510 - acc: 0.8241 - val_loss: 0.5486 - val_acc: 0.8250\n",
            "Epoch 75/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.5486 - acc: 0.8247 - val_loss: 0.5476 - val_acc: 0.8259\n",
            "Epoch 76/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.5470 - acc: 0.8247 - val_loss: 0.5466 - val_acc: 0.8238\n",
            "Epoch 77/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5445 - acc: 0.8251 - val_loss: 0.5444 - val_acc: 0.8258\n",
            "Epoch 78/100\n",
            "70000/70000 [==============================] - 18s 252us/step - loss: 0.5440 - acc: 0.8247 - val_loss: 0.5426 - val_acc: 0.8246\n",
            "Epoch 79/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5413 - acc: 0.8255 - val_loss: 0.5389 - val_acc: 0.8255\n",
            "Epoch 80/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.5398 - acc: 0.8256 - val_loss: 0.5422 - val_acc: 0.8229\n",
            "Epoch 81/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5387 - acc: 0.8258 - val_loss: 0.5380 - val_acc: 0.8259\n",
            "Epoch 82/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5368 - acc: 0.8261 - val_loss: 0.5353 - val_acc: 0.8266\n",
            "Epoch 83/100\n",
            "70000/70000 [==============================] - 17s 247us/step - loss: 0.5374 - acc: 0.8251 - val_loss: 0.5351 - val_acc: 0.8259\n",
            "Epoch 84/100\n",
            "70000/70000 [==============================] - 17s 241us/step - loss: 0.5352 - acc: 0.8257 - val_loss: 0.5336 - val_acc: 0.8262\n",
            "Epoch 85/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.5341 - acc: 0.8257 - val_loss: 0.5333 - val_acc: 0.8251\n",
            "Epoch 86/100\n",
            "70000/70000 [==============================] - 17s 242us/step - loss: 0.5325 - acc: 0.8260 - val_loss: 0.5316 - val_acc: 0.8250\n",
            "Epoch 87/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.5297 - acc: 0.8269 - val_loss: 0.5365 - val_acc: 0.8216\n",
            "Epoch 88/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5288 - acc: 0.8268 - val_loss: 0.5276 - val_acc: 0.8271\n",
            "Epoch 89/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5276 - acc: 0.8270 - val_loss: 0.5269 - val_acc: 0.8270\n",
            "Epoch 90/100\n",
            "70000/70000 [==============================] - 17s 243us/step - loss: 0.5272 - acc: 0.8269 - val_loss: 0.5249 - val_acc: 0.8275\n",
            "Epoch 91/100\n",
            "70000/70000 [==============================] - 17s 245us/step - loss: 0.5259 - acc: 0.8272 - val_loss: 0.5264 - val_acc: 0.8264\n",
            "Epoch 92/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5244 - acc: 0.8274 - val_loss: 0.5226 - val_acc: 0.8277\n",
            "Epoch 93/100\n",
            "70000/70000 [==============================] - 17s 246us/step - loss: 0.5230 - acc: 0.8276 - val_loss: 0.5275 - val_acc: 0.8252\n",
            "Epoch 94/100\n",
            "70000/70000 [==============================] - 17s 239us/step - loss: 0.5229 - acc: 0.8272 - val_loss: 0.5248 - val_acc: 0.8263\n",
            "Epoch 95/100\n",
            "70000/70000 [==============================] - 17s 242us/step - loss: 0.5212 - acc: 0.8277 - val_loss: 0.5204 - val_acc: 0.8266\n",
            "Epoch 96/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5200 - acc: 0.8280 - val_loss: 0.5295 - val_acc: 0.8238\n",
            "Epoch 97/100\n",
            "70000/70000 [==============================] - 17s 241us/step - loss: 0.5207 - acc: 0.8274 - val_loss: 0.5183 - val_acc: 0.8279\n",
            "Epoch 98/100\n",
            "70000/70000 [==============================] - 17s 244us/step - loss: 0.5190 - acc: 0.8278 - val_loss: 0.5173 - val_acc: 0.8280\n",
            "Epoch 99/100\n",
            "70000/70000 [==============================] - 17s 242us/step - loss: 0.5185 - acc: 0.8278 - val_loss: 0.5165 - val_acc: 0.8278\n",
            "Epoch 100/100\n",
            "70000/70000 [==============================] - 17s 248us/step - loss: 0.5166 - acc: 0.8282 - val_loss: 0.5185 - val_acc: 0.8268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m7cCtJMZPll"
      },
      "source": [
        "# loss: 0.2927 - acc: 0.9009 - val_loss: 0.2915 - val_acc: 0.9015"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXO4W0fqRQ6y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c185ccbe-34f0-46c4-f765-69f30e5eefcf"
      },
      "source": [
        "#%%timeit -n 1 -r 1\n",
        "history2 = model.fit(X, y_categorical, batch_size=128, epochs=1000, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 70000 samples, validate on 30000 samples\n",
            "Epoch 1/1000\n",
            "70000/70000 [==============================] - 66s 943us/step - loss: 0.5271 - acc: 0.8235 - val_loss: 0.5210 - val_acc: 0.8254\n",
            "Epoch 2/1000\n",
            "70000/70000 [==============================] - 66s 939us/step - loss: 0.5206 - acc: 0.8261 - val_loss: 0.5165 - val_acc: 0.8280\n",
            "Epoch 3/1000\n",
            "70000/70000 [==============================] - 67s 956us/step - loss: 0.5177 - acc: 0.8275 - val_loss: 0.5355 - val_acc: 0.8208\n",
            "Epoch 4/1000\n",
            "70000/70000 [==============================] - 66s 943us/step - loss: 0.5162 - acc: 0.8287 - val_loss: 0.5115 - val_acc: 0.8299\n",
            "Epoch 5/1000\n",
            "70000/70000 [==============================] - 66s 946us/step - loss: 0.5149 - acc: 0.8290 - val_loss: 0.5171 - val_acc: 0.8298\n",
            "Epoch 6/1000\n",
            "70000/70000 [==============================] - 66s 946us/step - loss: 0.5124 - acc: 0.8297 - val_loss: 0.5073 - val_acc: 0.8313\n",
            "Epoch 7/1000\n",
            "70000/70000 [==============================] - 67s 956us/step - loss: 0.5113 - acc: 0.8299 - val_loss: 0.5110 - val_acc: 0.8280\n",
            "Epoch 8/1000\n",
            "70000/70000 [==============================] - 67s 952us/step - loss: 0.5103 - acc: 0.8295 - val_loss: 0.5082 - val_acc: 0.8311\n",
            "Epoch 9/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.5090 - acc: 0.8299 - val_loss: 0.5131 - val_acc: 0.8287\n",
            "Epoch 10/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.5073 - acc: 0.8301 - val_loss: 0.5141 - val_acc: 0.8279\n",
            "Epoch 11/1000\n",
            "70000/70000 [==============================] - 65s 933us/step - loss: 0.5068 - acc: 0.8298 - val_loss: 0.5029 - val_acc: 0.8317\n",
            "Epoch 12/1000\n",
            "70000/70000 [==============================] - 66s 939us/step - loss: 0.5048 - acc: 0.8305 - val_loss: 0.5018 - val_acc: 0.8303\n",
            "Epoch 13/1000\n",
            "70000/70000 [==============================] - 67s 951us/step - loss: 0.5042 - acc: 0.8302 - val_loss: 0.5015 - val_acc: 0.8319\n",
            "Epoch 14/1000\n",
            "70000/70000 [==============================] - 66s 946us/step - loss: 0.5025 - acc: 0.8306 - val_loss: 0.5012 - val_acc: 0.8315\n",
            "Epoch 15/1000\n",
            "70000/70000 [==============================] - 65s 934us/step - loss: 0.5024 - acc: 0.8305 - val_loss: 0.5085 - val_acc: 0.8284\n",
            "Epoch 16/1000\n",
            "70000/70000 [==============================] - 65s 933us/step - loss: 0.5015 - acc: 0.8305 - val_loss: 0.4985 - val_acc: 0.8318\n",
            "Epoch 17/1000\n",
            "70000/70000 [==============================] - 65s 934us/step - loss: 0.5007 - acc: 0.8306 - val_loss: 0.5011 - val_acc: 0.8298\n",
            "Epoch 18/1000\n",
            "70000/70000 [==============================] - 66s 941us/step - loss: 0.4997 - acc: 0.8304 - val_loss: 0.4982 - val_acc: 0.8299\n",
            "Epoch 19/1000\n",
            "70000/70000 [==============================] - 65s 925us/step - loss: 0.4980 - acc: 0.8310 - val_loss: 0.5009 - val_acc: 0.8291\n",
            "Epoch 20/1000\n",
            "70000/70000 [==============================] - 65s 926us/step - loss: 0.4981 - acc: 0.8305 - val_loss: 0.4990 - val_acc: 0.8286\n",
            "Epoch 21/1000\n",
            "70000/70000 [==============================] - 65s 925us/step - loss: 0.4962 - acc: 0.8309 - val_loss: 0.4980 - val_acc: 0.8319\n",
            "Epoch 22/1000\n",
            "70000/70000 [==============================] - 65s 925us/step - loss: 0.4958 - acc: 0.8313 - val_loss: 0.4957 - val_acc: 0.8305\n",
            "Epoch 23/1000\n",
            "70000/70000 [==============================] - 65s 927us/step - loss: 0.4950 - acc: 0.8311 - val_loss: 0.4974 - val_acc: 0.8291\n",
            "Epoch 24/1000\n",
            "70000/70000 [==============================] - 64s 917us/step - loss: 0.4943 - acc: 0.8313 - val_loss: 0.4925 - val_acc: 0.8325\n",
            "Epoch 25/1000\n",
            "70000/70000 [==============================] - 64s 917us/step - loss: 0.4940 - acc: 0.8311 - val_loss: 0.4949 - val_acc: 0.8319\n",
            "Epoch 26/1000\n",
            "70000/70000 [==============================] - 65s 928us/step - loss: 0.4928 - acc: 0.8316 - val_loss: 0.4900 - val_acc: 0.8333\n",
            "Epoch 27/1000\n",
            "70000/70000 [==============================] - 65s 930us/step - loss: 0.4923 - acc: 0.8314 - val_loss: 0.4913 - val_acc: 0.8314\n",
            "Epoch 28/1000\n",
            "70000/70000 [==============================] - 65s 926us/step - loss: 0.4916 - acc: 0.8315 - val_loss: 0.4901 - val_acc: 0.8323\n",
            "Epoch 29/1000\n",
            "70000/70000 [==============================] - 64s 921us/step - loss: 0.4913 - acc: 0.8315 - val_loss: 0.4890 - val_acc: 0.8315\n",
            "Epoch 30/1000\n",
            "70000/70000 [==============================] - 64s 919us/step - loss: 0.4911 - acc: 0.8313 - val_loss: 0.4895 - val_acc: 0.8323\n",
            "Epoch 31/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4896 - acc: 0.8316 - val_loss: 0.4968 - val_acc: 0.8306\n",
            "Epoch 32/1000\n",
            "70000/70000 [==============================] - 64s 913us/step - loss: 0.4890 - acc: 0.8318 - val_loss: 0.4856 - val_acc: 0.8339\n",
            "Epoch 33/1000\n",
            "70000/70000 [==============================] - 65s 925us/step - loss: 0.4884 - acc: 0.8316 - val_loss: 0.4928 - val_acc: 0.8289\n",
            "Epoch 34/1000\n",
            "70000/70000 [==============================] - 64s 916us/step - loss: 0.4888 - acc: 0.8314 - val_loss: 0.4854 - val_acc: 0.8307\n",
            "Epoch 35/1000\n",
            "70000/70000 [==============================] - 65s 928us/step - loss: 0.4878 - acc: 0.8313 - val_loss: 0.4843 - val_acc: 0.8338\n",
            "Epoch 36/1000\n",
            "70000/70000 [==============================] - 64s 916us/step - loss: 0.4868 - acc: 0.8318 - val_loss: 0.4843 - val_acc: 0.8331\n",
            "Epoch 37/1000\n",
            "70000/70000 [==============================] - 65s 928us/step - loss: 0.4863 - acc: 0.8318 - val_loss: 0.4837 - val_acc: 0.8332\n",
            "Epoch 38/1000\n",
            "70000/70000 [==============================] - 65s 929us/step - loss: 0.4853 - acc: 0.8320 - val_loss: 0.4840 - val_acc: 0.8320\n",
            "Epoch 39/1000\n",
            "70000/70000 [==============================] - 65s 929us/step - loss: 0.4844 - acc: 0.8320 - val_loss: 0.4822 - val_acc: 0.8327\n",
            "Epoch 40/1000\n",
            "70000/70000 [==============================] - 65s 929us/step - loss: 0.4843 - acc: 0.8319 - val_loss: 0.4855 - val_acc: 0.8289\n",
            "Epoch 41/1000\n",
            "70000/70000 [==============================] - 65s 926us/step - loss: 0.4838 - acc: 0.8319 - val_loss: 0.4818 - val_acc: 0.8324\n",
            "Epoch 42/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.4834 - acc: 0.8320 - val_loss: 0.4836 - val_acc: 0.8313\n",
            "Epoch 43/1000\n",
            "70000/70000 [==============================] - 65s 933us/step - loss: 0.4826 - acc: 0.8319 - val_loss: 0.4797 - val_acc: 0.8335\n",
            "Epoch 44/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.4818 - acc: 0.8321 - val_loss: 0.4789 - val_acc: 0.8323\n",
            "Epoch 45/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.4820 - acc: 0.8319 - val_loss: 0.4843 - val_acc: 0.8312\n",
            "Epoch 46/1000\n",
            "70000/70000 [==============================] - 66s 938us/step - loss: 0.4818 - acc: 0.8318 - val_loss: 0.4821 - val_acc: 0.8318\n",
            "Epoch 47/1000\n",
            "70000/70000 [==============================] - 65s 932us/step - loss: 0.4809 - acc: 0.8322 - val_loss: 0.4807 - val_acc: 0.8320\n",
            "Epoch 48/1000\n",
            "70000/70000 [==============================] - 66s 938us/step - loss: 0.4812 - acc: 0.8319 - val_loss: 0.4787 - val_acc: 0.8325\n",
            "Epoch 49/1000\n",
            "70000/70000 [==============================] - 66s 943us/step - loss: 0.4797 - acc: 0.8324 - val_loss: 0.4794 - val_acc: 0.8320\n",
            "Epoch 50/1000\n",
            "70000/70000 [==============================] - 66s 941us/step - loss: 0.4792 - acc: 0.8323 - val_loss: 0.4850 - val_acc: 0.8302\n",
            "Epoch 51/1000\n",
            "70000/70000 [==============================] - 66s 941us/step - loss: 0.4795 - acc: 0.8319 - val_loss: 0.4779 - val_acc: 0.8319\n",
            "Epoch 52/1000\n",
            "70000/70000 [==============================] - 66s 948us/step - loss: 0.4792 - acc: 0.8320 - val_loss: 0.4760 - val_acc: 0.8341\n",
            "Epoch 53/1000\n",
            "70000/70000 [==============================] - 66s 945us/step - loss: 0.4778 - acc: 0.8324 - val_loss: 0.4812 - val_acc: 0.8311\n",
            "Epoch 54/1000\n",
            "70000/70000 [==============================] - 66s 946us/step - loss: 0.4783 - acc: 0.8318 - val_loss: 0.4765 - val_acc: 0.8324\n",
            "Epoch 55/1000\n",
            "70000/70000 [==============================] - 67s 954us/step - loss: 0.4785 - acc: 0.8319 - val_loss: 0.4770 - val_acc: 0.8304\n",
            "Epoch 56/1000\n",
            "70000/70000 [==============================] - 70s 997us/step - loss: 0.4764 - acc: 0.8324 - val_loss: 0.4862 - val_acc: 0.8282\n",
            "Epoch 57/1000\n",
            "70000/70000 [==============================] - 68s 967us/step - loss: 0.4771 - acc: 0.8322 - val_loss: 0.4804 - val_acc: 0.8302\n",
            "Epoch 58/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4764 - acc: 0.8323 - val_loss: 0.4791 - val_acc: 0.8309\n",
            "Epoch 59/1000\n",
            "70000/70000 [==============================] - 68s 967us/step - loss: 0.4753 - acc: 0.8325 - val_loss: 0.4760 - val_acc: 0.8324\n",
            "Epoch 60/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4759 - acc: 0.8325 - val_loss: 0.4782 - val_acc: 0.8309\n",
            "Epoch 61/1000\n",
            "70000/70000 [==============================] - 67s 963us/step - loss: 0.4753 - acc: 0.8323 - val_loss: 0.4736 - val_acc: 0.8320\n",
            "Epoch 62/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4749 - acc: 0.8322 - val_loss: 0.4748 - val_acc: 0.8312\n",
            "Epoch 63/1000\n",
            "70000/70000 [==============================] - 68s 969us/step - loss: 0.4745 - acc: 0.8324 - val_loss: 0.4722 - val_acc: 0.8328\n",
            "Epoch 64/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4741 - acc: 0.8325 - val_loss: 0.4726 - val_acc: 0.8331\n",
            "Epoch 65/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4739 - acc: 0.8323 - val_loss: 0.4709 - val_acc: 0.8316\n",
            "Epoch 66/1000\n",
            "70000/70000 [==============================] - 68s 966us/step - loss: 0.4732 - acc: 0.8323 - val_loss: 0.4754 - val_acc: 0.8314\n",
            "Epoch 67/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4731 - acc: 0.8323 - val_loss: 0.4761 - val_acc: 0.8316\n",
            "Epoch 68/1000\n",
            "70000/70000 [==============================] - 68s 967us/step - loss: 0.4733 - acc: 0.8324 - val_loss: 0.4787 - val_acc: 0.8295\n",
            "Epoch 69/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4729 - acc: 0.8324 - val_loss: 0.4718 - val_acc: 0.8335\n",
            "Epoch 70/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4723 - acc: 0.8324 - val_loss: 0.4694 - val_acc: 0.8343\n",
            "Epoch 71/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4706 - acc: 0.8332 - val_loss: 0.4748 - val_acc: 0.8324\n",
            "Epoch 72/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4719 - acc: 0.8326 - val_loss: 0.4695 - val_acc: 0.8334\n",
            "Epoch 73/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4721 - acc: 0.8325 - val_loss: 0.4708 - val_acc: 0.8330\n",
            "Epoch 74/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4707 - acc: 0.8331 - val_loss: 0.4737 - val_acc: 0.8310\n",
            "Epoch 75/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4711 - acc: 0.8326 - val_loss: 0.4698 - val_acc: 0.8330\n",
            "Epoch 76/1000\n",
            "70000/70000 [==============================] - 68s 969us/step - loss: 0.4707 - acc: 0.8329 - val_loss: 0.4750 - val_acc: 0.8318\n",
            "Epoch 77/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4707 - acc: 0.8328 - val_loss: 0.4682 - val_acc: 0.8353\n",
            "Epoch 78/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4702 - acc: 0.8329 - val_loss: 0.4678 - val_acc: 0.8337\n",
            "Epoch 79/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4696 - acc: 0.8328 - val_loss: 0.4735 - val_acc: 0.8311\n",
            "Epoch 80/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4692 - acc: 0.8330 - val_loss: 0.4668 - val_acc: 0.8341\n",
            "Epoch 81/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4691 - acc: 0.8331 - val_loss: 0.4735 - val_acc: 0.8324\n",
            "Epoch 82/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4699 - acc: 0.8329 - val_loss: 0.4719 - val_acc: 0.8312\n",
            "Epoch 83/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4686 - acc: 0.8332 - val_loss: 0.4651 - val_acc: 0.8333\n",
            "Epoch 84/1000\n",
            "70000/70000 [==============================] - 69s 986us/step - loss: 0.4685 - acc: 0.8331 - val_loss: 0.4666 - val_acc: 0.8341\n",
            "Epoch 85/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4678 - acc: 0.8333 - val_loss: 0.4683 - val_acc: 0.8330\n",
            "Epoch 86/1000\n",
            "70000/70000 [==============================] - 68s 969us/step - loss: 0.4677 - acc: 0.8333 - val_loss: 0.4668 - val_acc: 0.8318\n",
            "Epoch 87/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4678 - acc: 0.8330 - val_loss: 0.4665 - val_acc: 0.8332\n",
            "Epoch 88/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4677 - acc: 0.8330 - val_loss: 0.4656 - val_acc: 0.8334\n",
            "Epoch 89/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4666 - acc: 0.8334 - val_loss: 0.4680 - val_acc: 0.8337\n",
            "Epoch 90/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4676 - acc: 0.8330 - val_loss: 0.4687 - val_acc: 0.8308\n",
            "Epoch 91/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4670 - acc: 0.8333 - val_loss: 0.4676 - val_acc: 0.8325\n",
            "Epoch 92/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4666 - acc: 0.8332 - val_loss: 0.4650 - val_acc: 0.8337\n",
            "Epoch 93/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4662 - acc: 0.8333 - val_loss: 0.4662 - val_acc: 0.8337\n",
            "Epoch 94/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4658 - acc: 0.8334 - val_loss: 0.4657 - val_acc: 0.8318\n",
            "Epoch 95/1000\n",
            "70000/70000 [==============================] - 68s 969us/step - loss: 0.4657 - acc: 0.8334 - val_loss: 0.4678 - val_acc: 0.8327\n",
            "Epoch 96/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4655 - acc: 0.8335 - val_loss: 0.4646 - val_acc: 0.8341\n",
            "Epoch 97/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4657 - acc: 0.8333 - val_loss: 0.4641 - val_acc: 0.8339\n",
            "Epoch 98/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4657 - acc: 0.8332 - val_loss: 0.4676 - val_acc: 0.8317\n",
            "Epoch 99/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4659 - acc: 0.8330 - val_loss: 0.4645 - val_acc: 0.8334\n",
            "Epoch 100/1000\n",
            "70000/70000 [==============================] - 68s 965us/step - loss: 0.4648 - acc: 0.8334 - val_loss: 0.4679 - val_acc: 0.8327\n",
            "Epoch 101/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4655 - acc: 0.8330 - val_loss: 0.4637 - val_acc: 0.8344\n",
            "Epoch 102/1000\n",
            "70000/70000 [==============================] - 69s 982us/step - loss: 0.4650 - acc: 0.8334 - val_loss: 0.4657 - val_acc: 0.8342\n",
            "Epoch 103/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4642 - acc: 0.8335 - val_loss: 0.4648 - val_acc: 0.8325\n",
            "Epoch 104/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4648 - acc: 0.8334 - val_loss: 0.4619 - val_acc: 0.8340\n",
            "Epoch 105/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4635 - acc: 0.8336 - val_loss: 0.4642 - val_acc: 0.8326\n",
            "Epoch 106/1000\n",
            "70000/70000 [==============================] - 69s 985us/step - loss: 0.4641 - acc: 0.8332 - val_loss: 0.4635 - val_acc: 0.8327\n",
            "Epoch 107/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4635 - acc: 0.8333 - val_loss: 0.4661 - val_acc: 0.8326\n",
            "Epoch 108/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4635 - acc: 0.8336 - val_loss: 0.4615 - val_acc: 0.8336\n",
            "Epoch 109/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4633 - acc: 0.8334 - val_loss: 0.4635 - val_acc: 0.8327\n",
            "Epoch 110/1000\n",
            "70000/70000 [==============================] - 69s 979us/step - loss: 0.4630 - acc: 0.8333 - val_loss: 0.4620 - val_acc: 0.8329\n",
            "Epoch 111/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4631 - acc: 0.8334 - val_loss: 0.4619 - val_acc: 0.8343\n",
            "Epoch 112/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4626 - acc: 0.8336 - val_loss: 0.4684 - val_acc: 0.8314\n",
            "Epoch 113/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4623 - acc: 0.8337 - val_loss: 0.4633 - val_acc: 0.8323\n",
            "Epoch 114/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4624 - acc: 0.8336 - val_loss: 0.4612 - val_acc: 0.8336\n",
            "Epoch 115/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4622 - acc: 0.8335 - val_loss: 0.4684 - val_acc: 0.8334\n",
            "Epoch 116/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4622 - acc: 0.8335 - val_loss: 0.4619 - val_acc: 0.8325\n",
            "Epoch 117/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4619 - acc: 0.8334 - val_loss: 0.4649 - val_acc: 0.8331\n",
            "Epoch 118/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4620 - acc: 0.8336 - val_loss: 0.4655 - val_acc: 0.8335\n",
            "Epoch 119/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4616 - acc: 0.8335 - val_loss: 0.4602 - val_acc: 0.8343\n",
            "Epoch 120/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4611 - acc: 0.8333 - val_loss: 0.4619 - val_acc: 0.8328\n",
            "Epoch 121/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4613 - acc: 0.8336 - val_loss: 0.4611 - val_acc: 0.8337\n",
            "Epoch 122/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4611 - acc: 0.8337 - val_loss: 0.4599 - val_acc: 0.8334\n",
            "Epoch 123/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4613 - acc: 0.8333 - val_loss: 0.4608 - val_acc: 0.8345\n",
            "Epoch 124/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4611 - acc: 0.8335 - val_loss: 0.4619 - val_acc: 0.8342\n",
            "Epoch 125/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4604 - acc: 0.8336 - val_loss: 0.4602 - val_acc: 0.8329\n",
            "Epoch 126/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4605 - acc: 0.8336 - val_loss: 0.4615 - val_acc: 0.8325\n",
            "Epoch 127/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4602 - acc: 0.8334 - val_loss: 0.4584 - val_acc: 0.8340\n",
            "Epoch 128/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4601 - acc: 0.8338 - val_loss: 0.4613 - val_acc: 0.8343\n",
            "Epoch 129/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4601 - acc: 0.8335 - val_loss: 0.4624 - val_acc: 0.8329\n",
            "Epoch 130/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4598 - acc: 0.8335 - val_loss: 0.4587 - val_acc: 0.8345\n",
            "Epoch 131/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4595 - acc: 0.8334 - val_loss: 0.4589 - val_acc: 0.8335\n",
            "Epoch 132/1000\n",
            "70000/70000 [==============================] - 68s 969us/step - loss: 0.4593 - acc: 0.8339 - val_loss: 0.4582 - val_acc: 0.8345\n",
            "Epoch 133/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4596 - acc: 0.8335 - val_loss: 0.4583 - val_acc: 0.8340\n",
            "Epoch 134/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4592 - acc: 0.8337 - val_loss: 0.4604 - val_acc: 0.8340\n",
            "Epoch 135/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4593 - acc: 0.8337 - val_loss: 0.4605 - val_acc: 0.8325\n",
            "Epoch 136/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4588 - acc: 0.8336 - val_loss: 0.4582 - val_acc: 0.8339\n",
            "Epoch 137/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4586 - acc: 0.8335 - val_loss: 0.4593 - val_acc: 0.8339\n",
            "Epoch 138/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4588 - acc: 0.8335 - val_loss: 0.4581 - val_acc: 0.8316\n",
            "Epoch 139/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4586 - acc: 0.8335 - val_loss: 0.4582 - val_acc: 0.8335\n",
            "Epoch 140/1000\n",
            "70000/70000 [==============================] - 67s 963us/step - loss: 0.4587 - acc: 0.8335 - val_loss: 0.4562 - val_acc: 0.8348\n",
            "Epoch 141/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4579 - acc: 0.8338 - val_loss: 0.4566 - val_acc: 0.8341\n",
            "Epoch 142/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4579 - acc: 0.8338 - val_loss: 0.4584 - val_acc: 0.8327\n",
            "Epoch 143/1000\n",
            "70000/70000 [==============================] - 69s 979us/step - loss: 0.4576 - acc: 0.8337 - val_loss: 0.4617 - val_acc: 0.8319\n",
            "Epoch 144/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4577 - acc: 0.8337 - val_loss: 0.4579 - val_acc: 0.8338\n",
            "Epoch 145/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4578 - acc: 0.8337 - val_loss: 0.4567 - val_acc: 0.8337\n",
            "Epoch 146/1000\n",
            "70000/70000 [==============================] - 67s 960us/step - loss: 0.4572 - acc: 0.8337 - val_loss: 0.4567 - val_acc: 0.8350\n",
            "Epoch 147/1000\n",
            "70000/70000 [==============================] - 67s 959us/step - loss: 0.4576 - acc: 0.8335 - val_loss: 0.4578 - val_acc: 0.8328\n",
            "Epoch 148/1000\n",
            "70000/70000 [==============================] - 69s 983us/step - loss: 0.4576 - acc: 0.8336 - val_loss: 0.4567 - val_acc: 0.8335\n",
            "Epoch 149/1000\n",
            "70000/70000 [==============================] - 69s 982us/step - loss: 0.4568 - acc: 0.8339 - val_loss: 0.4579 - val_acc: 0.8332\n",
            "Epoch 150/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4573 - acc: 0.8334 - val_loss: 0.4588 - val_acc: 0.8328\n",
            "Epoch 151/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4569 - acc: 0.8338 - val_loss: 0.4565 - val_acc: 0.8336\n",
            "Epoch 152/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4565 - acc: 0.8339 - val_loss: 0.4568 - val_acc: 0.8342\n",
            "Epoch 153/1000\n",
            "70000/70000 [==============================] - 68s 966us/step - loss: 0.4566 - acc: 0.8336 - val_loss: 0.4567 - val_acc: 0.8341\n",
            "Epoch 154/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4565 - acc: 0.8337 - val_loss: 0.4600 - val_acc: 0.8326\n",
            "Epoch 155/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4561 - acc: 0.8340 - val_loss: 0.4576 - val_acc: 0.8343\n",
            "Epoch 156/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4562 - acc: 0.8340 - val_loss: 0.4567 - val_acc: 0.8339\n",
            "Epoch 157/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4564 - acc: 0.8339 - val_loss: 0.4577 - val_acc: 0.8338\n",
            "Epoch 158/1000\n",
            "70000/70000 [==============================] - 69s 984us/step - loss: 0.4562 - acc: 0.8337 - val_loss: 0.4575 - val_acc: 0.8315\n",
            "Epoch 159/1000\n",
            "70000/70000 [==============================] - 71s 1ms/step - loss: 0.4562 - acc: 0.8338 - val_loss: 0.4551 - val_acc: 0.8350\n",
            "Epoch 160/1000\n",
            "70000/70000 [==============================] - 70s 994us/step - loss: 0.4560 - acc: 0.8337 - val_loss: 0.4549 - val_acc: 0.8337\n",
            "Epoch 161/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4559 - acc: 0.8338 - val_loss: 0.4541 - val_acc: 0.8348\n",
            "Epoch 162/1000\n",
            "70000/70000 [==============================] - 69s 983us/step - loss: 0.4555 - acc: 0.8340 - val_loss: 0.4564 - val_acc: 0.8335\n",
            "Epoch 163/1000\n",
            "70000/70000 [==============================] - 69s 983us/step - loss: 0.4557 - acc: 0.8338 - val_loss: 0.4558 - val_acc: 0.8341\n",
            "Epoch 164/1000\n",
            "70000/70000 [==============================] - 69s 979us/step - loss: 0.4558 - acc: 0.8337 - val_loss: 0.4568 - val_acc: 0.8339\n",
            "Epoch 165/1000\n",
            "70000/70000 [==============================] - 69s 984us/step - loss: 0.4555 - acc: 0.8339 - val_loss: 0.4573 - val_acc: 0.8337\n",
            "Epoch 166/1000\n",
            "70000/70000 [==============================] - 69s 982us/step - loss: 0.4551 - acc: 0.8339 - val_loss: 0.4551 - val_acc: 0.8342\n",
            "Epoch 167/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4551 - acc: 0.8339 - val_loss: 0.4557 - val_acc: 0.8338\n",
            "Epoch 168/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4549 - acc: 0.8340 - val_loss: 0.4563 - val_acc: 0.8339\n",
            "Epoch 169/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4549 - acc: 0.8340 - val_loss: 0.4564 - val_acc: 0.8340\n",
            "Epoch 170/1000\n",
            "70000/70000 [==============================] - 69s 985us/step - loss: 0.4548 - acc: 0.8339 - val_loss: 0.4637 - val_acc: 0.8280\n",
            "Epoch 171/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4550 - acc: 0.8338 - val_loss: 0.4538 - val_acc: 0.8330\n",
            "Epoch 172/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4554 - acc: 0.8339 - val_loss: 0.4562 - val_acc: 0.8337\n",
            "Epoch 173/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4548 - acc: 0.8338 - val_loss: 0.4545 - val_acc: 0.8348\n",
            "Epoch 174/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4546 - acc: 0.8342 - val_loss: 0.4551 - val_acc: 0.8334\n",
            "Epoch 175/1000\n",
            "70000/70000 [==============================] - 68s 975us/step - loss: 0.4547 - acc: 0.8337 - val_loss: 0.4545 - val_acc: 0.8345\n",
            "Epoch 176/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4544 - acc: 0.8339 - val_loss: 0.4530 - val_acc: 0.8346\n",
            "Epoch 177/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4544 - acc: 0.8340 - val_loss: 0.4561 - val_acc: 0.8326\n",
            "Epoch 178/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4545 - acc: 0.8341 - val_loss: 0.4559 - val_acc: 0.8322\n",
            "Epoch 179/1000\n",
            "70000/70000 [==============================] - 69s 987us/step - loss: 0.4547 - acc: 0.8338 - val_loss: 0.4534 - val_acc: 0.8337\n",
            "Epoch 180/1000\n",
            "70000/70000 [==============================] - 69s 983us/step - loss: 0.4541 - acc: 0.8341 - val_loss: 0.4541 - val_acc: 0.8330\n",
            "Epoch 181/1000\n",
            "70000/70000 [==============================] - 69s 982us/step - loss: 0.4543 - acc: 0.8338 - val_loss: 0.4565 - val_acc: 0.8324\n",
            "Epoch 182/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4542 - acc: 0.8338 - val_loss: 0.4559 - val_acc: 0.8327\n",
            "Epoch 183/1000\n",
            "70000/70000 [==============================] - 69s 979us/step - loss: 0.4540 - acc: 0.8340 - val_loss: 0.4548 - val_acc: 0.8335\n",
            "Epoch 184/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4544 - acc: 0.8339 - val_loss: 0.4535 - val_acc: 0.8340\n",
            "Epoch 185/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4540 - acc: 0.8339 - val_loss: 0.4560 - val_acc: 0.8339\n",
            "Epoch 186/1000\n",
            "70000/70000 [==============================] - 68s 976us/step - loss: 0.4539 - acc: 0.8339 - val_loss: 0.4528 - val_acc: 0.8333\n",
            "Epoch 187/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4535 - acc: 0.8340 - val_loss: 0.4537 - val_acc: 0.8349\n",
            "Epoch 188/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4539 - acc: 0.8338 - val_loss: 0.4541 - val_acc: 0.8336\n",
            "Epoch 189/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4535 - acc: 0.8338 - val_loss: 0.4568 - val_acc: 0.8322\n",
            "Epoch 190/1000\n",
            "70000/70000 [==============================] - 67s 963us/step - loss: 0.4538 - acc: 0.8337 - val_loss: 0.4549 - val_acc: 0.8331\n",
            "Epoch 191/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4537 - acc: 0.8339 - val_loss: 0.4558 - val_acc: 0.8336\n",
            "Epoch 192/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4531 - acc: 0.8340 - val_loss: 0.4522 - val_acc: 0.8344\n",
            "Epoch 193/1000\n",
            "70000/70000 [==============================] - 69s 983us/step - loss: 0.4534 - acc: 0.8342 - val_loss: 0.4542 - val_acc: 0.8330\n",
            "Epoch 194/1000\n",
            "70000/70000 [==============================] - 69s 982us/step - loss: 0.4533 - acc: 0.8340 - val_loss: 0.4532 - val_acc: 0.8348\n",
            "Epoch 195/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4532 - acc: 0.8341 - val_loss: 0.4537 - val_acc: 0.8334\n",
            "Epoch 196/1000\n",
            "70000/70000 [==============================] - 68s 977us/step - loss: 0.4534 - acc: 0.8337 - val_loss: 0.4550 - val_acc: 0.8345\n",
            "Epoch 197/1000\n",
            "70000/70000 [==============================] - 69s 980us/step - loss: 0.4530 - acc: 0.8343 - val_loss: 0.4510 - val_acc: 0.8354\n",
            "Epoch 198/1000\n",
            "70000/70000 [==============================] - 69s 979us/step - loss: 0.4533 - acc: 0.8340 - val_loss: 0.4530 - val_acc: 0.8349\n",
            "Epoch 199/1000\n",
            "70000/70000 [==============================] - 69s 981us/step - loss: 0.4534 - acc: 0.8338 - val_loss: 0.4521 - val_acc: 0.8349\n",
            "Epoch 200/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4529 - acc: 0.8341 - val_loss: 0.4544 - val_acc: 0.8334\n",
            "Epoch 201/1000\n",
            "70000/70000 [==============================] - 68s 970us/step - loss: 0.4530 - acc: 0.8341 - val_loss: 0.4555 - val_acc: 0.8336\n",
            "Epoch 202/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4532 - acc: 0.8340 - val_loss: 0.4547 - val_acc: 0.8344\n",
            "Epoch 203/1000\n",
            "70000/70000 [==============================] - 68s 971us/step - loss: 0.4530 - acc: 0.8340 - val_loss: 0.4524 - val_acc: 0.8341\n",
            "Epoch 204/1000\n",
            "70000/70000 [==============================] - 68s 973us/step - loss: 0.4528 - acc: 0.8343 - val_loss: 0.4515 - val_acc: 0.8332\n",
            "Epoch 205/1000\n",
            "70000/70000 [==============================] - 68s 968us/step - loss: 0.4527 - acc: 0.8340 - val_loss: 0.4538 - val_acc: 0.8341\n",
            "Epoch 206/1000\n",
            "70000/70000 [==============================] - 69s 985us/step - loss: 0.4523 - acc: 0.8341 - val_loss: 0.4522 - val_acc: 0.8347\n",
            "Epoch 207/1000\n",
            "70000/70000 [==============================] - 68s 974us/step - loss: 0.4524 - acc: 0.8340 - val_loss: 0.4531 - val_acc: 0.8341\n",
            "Epoch 208/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4527 - acc: 0.8341 - val_loss: 0.4534 - val_acc: 0.8342\n",
            "Epoch 209/1000\n",
            "70000/70000 [==============================] - 68s 972us/step - loss: 0.4523 - acc: 0.8343 - val_loss: 0.4522 - val_acc: 0.8340\n",
            "Epoch 210/1000\n",
            "70000/70000 [==============================] - 68s 978us/step - loss: 0.4524 - acc: 0.8337 - val_loss: 0.4520 - val_acc: 0.8338\n",
            "Epoch 211/1000\n",
            "70000/70000 [==============================] - 68s 967us/step - loss: 0.4520 - acc: 0.8342 - val_loss: 0.4518 - val_acc: 0.8334\n",
            "Epoch 212/1000\n",
            "70000/70000 [==============================] - 67s 950us/step - loss: 0.4525 - acc: 0.8340 - val_loss: 0.4536 - val_acc: 0.8344\n",
            "Epoch 213/1000\n",
            "70000/70000 [==============================] - 66s 942us/step - loss: 0.4521 - acc: 0.8343 - val_loss: 0.4521 - val_acc: 0.8342\n",
            "Epoch 214/1000\n",
            "70000/70000 [==============================] - 66s 937us/step - loss: 0.4524 - acc: 0.8339 - val_loss: 0.4529 - val_acc: 0.8343\n",
            "Epoch 215/1000\n",
            "70000/70000 [==============================] - 65s 925us/step - loss: 0.4518 - acc: 0.8343 - val_loss: 0.4524 - val_acc: 0.8330\n",
            "Epoch 216/1000\n",
            "70000/70000 [==============================] - 64s 919us/step - loss: 0.4518 - acc: 0.8342 - val_loss: 0.4517 - val_acc: 0.8339\n",
            "Epoch 217/1000\n",
            "70000/70000 [==============================] - 64s 915us/step - loss: 0.4524 - acc: 0.8340 - val_loss: 0.4560 - val_acc: 0.8343\n",
            "Epoch 218/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4520 - acc: 0.8340 - val_loss: 0.4517 - val_acc: 0.8348\n",
            "Epoch 219/1000\n",
            "70000/70000 [==============================] - 64s 915us/step - loss: 0.4518 - acc: 0.8341 - val_loss: 0.4522 - val_acc: 0.8336\n",
            "Epoch 220/1000\n",
            "70000/70000 [==============================] - 64s 917us/step - loss: 0.4518 - acc: 0.8339 - val_loss: 0.4514 - val_acc: 0.8331\n",
            "Epoch 221/1000\n",
            "70000/70000 [==============================] - 64s 918us/step - loss: 0.4513 - acc: 0.8343 - val_loss: 0.4516 - val_acc: 0.8340\n",
            "Epoch 222/1000\n",
            "70000/70000 [==============================] - 65s 932us/step - loss: 0.4520 - acc: 0.8340 - val_loss: 0.4536 - val_acc: 0.8348\n",
            "Epoch 223/1000\n",
            "70000/70000 [==============================] - 65s 922us/step - loss: 0.4516 - acc: 0.8341 - val_loss: 0.4517 - val_acc: 0.8346\n",
            "Epoch 224/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4513 - acc: 0.8342 - val_loss: 0.4514 - val_acc: 0.8341\n",
            "Epoch 225/1000\n",
            "70000/70000 [==============================] - 64s 916us/step - loss: 0.4520 - acc: 0.8338 - val_loss: 0.4521 - val_acc: 0.8336\n",
            "Epoch 226/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4512 - acc: 0.8341 - val_loss: 0.4499 - val_acc: 0.8358\n",
            "Epoch 227/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4514 - acc: 0.8343 - val_loss: 0.4537 - val_acc: 0.8340\n",
            "Epoch 228/1000\n",
            "70000/70000 [==============================] - 64s 913us/step - loss: 0.4519 - acc: 0.8340 - val_loss: 0.4510 - val_acc: 0.8347\n",
            "Epoch 229/1000\n",
            "70000/70000 [==============================] - 64s 914us/step - loss: 0.4512 - acc: 0.8342 - val_loss: 0.4536 - val_acc: 0.8336\n",
            "Epoch 230/1000\n",
            "70000/70000 [==============================] - 64s 917us/step - loss: 0.4514 - acc: 0.8342 - val_loss: 0.4506 - val_acc: 0.8345\n",
            "Epoch 231/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4516 - acc: 0.8339 - val_loss: 0.4511 - val_acc: 0.8341\n",
            "Epoch 232/1000\n",
            "70000/70000 [==============================] - 64s 914us/step - loss: 0.4511 - acc: 0.8344 - val_loss: 0.4512 - val_acc: 0.8339\n",
            "Epoch 233/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4512 - acc: 0.8340 - val_loss: 0.4535 - val_acc: 0.8329\n",
            "Epoch 234/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4513 - acc: 0.8342 - val_loss: 0.4529 - val_acc: 0.8337\n",
            "Epoch 235/1000\n",
            "70000/70000 [==============================] - 64s 914us/step - loss: 0.4510 - acc: 0.8340 - val_loss: 0.4505 - val_acc: 0.8343\n",
            "Epoch 236/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4510 - acc: 0.8341 - val_loss: 0.4526 - val_acc: 0.8357\n",
            "Epoch 237/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4513 - acc: 0.8340 - val_loss: 0.4519 - val_acc: 0.8333\n",
            "Epoch 238/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4506 - acc: 0.8343 - val_loss: 0.4501 - val_acc: 0.8326\n",
            "Epoch 239/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4508 - acc: 0.8343 - val_loss: 0.4500 - val_acc: 0.8342\n",
            "Epoch 240/1000\n",
            "70000/70000 [==============================] - 64s 913us/step - loss: 0.4505 - acc: 0.8342 - val_loss: 0.4516 - val_acc: 0.8339\n",
            "Epoch 241/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4509 - acc: 0.8341 - val_loss: 0.4527 - val_acc: 0.8337\n",
            "Epoch 242/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4507 - acc: 0.8341 - val_loss: 0.4523 - val_acc: 0.8339\n",
            "Epoch 243/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4507 - acc: 0.8340 - val_loss: 0.4525 - val_acc: 0.8345\n",
            "Epoch 244/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4509 - acc: 0.8344 - val_loss: 0.4526 - val_acc: 0.8319\n",
            "Epoch 245/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4507 - acc: 0.8342 - val_loss: 0.4509 - val_acc: 0.8333\n",
            "Epoch 246/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4508 - acc: 0.8342 - val_loss: 0.4502 - val_acc: 0.8346\n",
            "Epoch 247/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4504 - acc: 0.8341 - val_loss: 0.4507 - val_acc: 0.8346\n",
            "Epoch 248/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4505 - acc: 0.8342 - val_loss: 0.4513 - val_acc: 0.8343\n",
            "Epoch 249/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4504 - acc: 0.8341 - val_loss: 0.4518 - val_acc: 0.8344\n",
            "Epoch 250/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4504 - acc: 0.8340 - val_loss: 0.4502 - val_acc: 0.8356\n",
            "Epoch 251/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4505 - acc: 0.8342 - val_loss: 0.4499 - val_acc: 0.8343\n",
            "Epoch 252/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4503 - acc: 0.8340 - val_loss: 0.4505 - val_acc: 0.8353\n",
            "Epoch 253/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4501 - acc: 0.8342 - val_loss: 0.4508 - val_acc: 0.8343\n",
            "Epoch 254/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4501 - acc: 0.8342 - val_loss: 0.4509 - val_acc: 0.8347\n",
            "Epoch 255/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4501 - acc: 0.8341 - val_loss: 0.4510 - val_acc: 0.8355\n",
            "Epoch 256/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4502 - acc: 0.8341 - val_loss: 0.4528 - val_acc: 0.8336\n",
            "Epoch 257/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4502 - acc: 0.8341 - val_loss: 0.4511 - val_acc: 0.8336\n",
            "Epoch 258/1000\n",
            "70000/70000 [==============================] - 63s 899us/step - loss: 0.4503 - acc: 0.8340 - val_loss: 0.4508 - val_acc: 0.8352\n",
            "Epoch 259/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4504 - acc: 0.8341 - val_loss: 0.4494 - val_acc: 0.8354\n",
            "Epoch 260/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4502 - acc: 0.8338 - val_loss: 0.4495 - val_acc: 0.8336\n",
            "Epoch 261/1000\n",
            "70000/70000 [==============================] - 63s 900us/step - loss: 0.4499 - acc: 0.8343 - val_loss: 0.4515 - val_acc: 0.8341\n",
            "Epoch 262/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4497 - acc: 0.8340 - val_loss: 0.4514 - val_acc: 0.8320\n",
            "Epoch 263/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4500 - acc: 0.8341 - val_loss: 0.4502 - val_acc: 0.8361\n",
            "Epoch 264/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4495 - acc: 0.8344 - val_loss: 0.4495 - val_acc: 0.8352\n",
            "Epoch 265/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4498 - acc: 0.8342 - val_loss: 0.4502 - val_acc: 0.8328\n",
            "Epoch 266/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4498 - acc: 0.8343 - val_loss: 0.4507 - val_acc: 0.8330\n",
            "Epoch 267/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4495 - acc: 0.8343 - val_loss: 0.4497 - val_acc: 0.8324\n",
            "Epoch 268/1000\n",
            "70000/70000 [==============================] - 63s 899us/step - loss: 0.4497 - acc: 0.8341 - val_loss: 0.4504 - val_acc: 0.8346\n",
            "Epoch 269/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4496 - acc: 0.8341 - val_loss: 0.4505 - val_acc: 0.8336\n",
            "Epoch 270/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4496 - acc: 0.8342 - val_loss: 0.4505 - val_acc: 0.8338\n",
            "Epoch 271/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4494 - acc: 0.8343 - val_loss: 0.4497 - val_acc: 0.8337\n",
            "Epoch 272/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4493 - acc: 0.8341 - val_loss: 0.4510 - val_acc: 0.8337\n",
            "Epoch 273/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4494 - acc: 0.8343 - val_loss: 0.4499 - val_acc: 0.8337\n",
            "Epoch 274/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4492 - acc: 0.8343 - val_loss: 0.4486 - val_acc: 0.8354\n",
            "Epoch 275/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4495 - acc: 0.8342 - val_loss: 0.4483 - val_acc: 0.8324\n",
            "Epoch 276/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4498 - acc: 0.8337 - val_loss: 0.4474 - val_acc: 0.8358\n",
            "Epoch 277/1000\n",
            "70000/70000 [==============================] - 63s 900us/step - loss: 0.4492 - acc: 0.8341 - val_loss: 0.4527 - val_acc: 0.8302\n",
            "Epoch 278/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4495 - acc: 0.8340 - val_loss: 0.4503 - val_acc: 0.8350\n",
            "Epoch 279/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4491 - acc: 0.8341 - val_loss: 0.4519 - val_acc: 0.8343\n",
            "Epoch 280/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4493 - acc: 0.8340 - val_loss: 0.4486 - val_acc: 0.8359\n",
            "Epoch 281/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4491 - acc: 0.8342 - val_loss: 0.4489 - val_acc: 0.8349\n",
            "Epoch 282/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4491 - acc: 0.8343 - val_loss: 0.4512 - val_acc: 0.8333\n",
            "Epoch 283/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4489 - acc: 0.8343 - val_loss: 0.4528 - val_acc: 0.8338\n",
            "Epoch 284/1000\n",
            "70000/70000 [==============================] - 64s 914us/step - loss: 0.4496 - acc: 0.8340 - val_loss: 0.4512 - val_acc: 0.8344\n",
            "Epoch 285/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4488 - acc: 0.8343 - val_loss: 0.4492 - val_acc: 0.8358\n",
            "Epoch 286/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4487 - acc: 0.8344 - val_loss: 0.4481 - val_acc: 0.8340\n",
            "Epoch 287/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4489 - acc: 0.8343 - val_loss: 0.4482 - val_acc: 0.8351\n",
            "Epoch 288/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4489 - acc: 0.8341 - val_loss: 0.4502 - val_acc: 0.8341\n",
            "Epoch 289/1000\n",
            "70000/70000 [==============================] - 64s 912us/step - loss: 0.4490 - acc: 0.8342 - val_loss: 0.4511 - val_acc: 0.8327\n",
            "Epoch 290/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4488 - acc: 0.8344 - val_loss: 0.4506 - val_acc: 0.8334\n",
            "Epoch 291/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4486 - acc: 0.8343 - val_loss: 0.4504 - val_acc: 0.8340\n",
            "Epoch 292/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4485 - acc: 0.8342 - val_loss: 0.4482 - val_acc: 0.8334\n",
            "Epoch 293/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4486 - acc: 0.8343 - val_loss: 0.4489 - val_acc: 0.8344\n",
            "Epoch 294/1000\n",
            "70000/70000 [==============================] - 64s 914us/step - loss: 0.4488 - acc: 0.8340 - val_loss: 0.4482 - val_acc: 0.8342\n",
            "Epoch 295/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4486 - acc: 0.8342 - val_loss: 0.4506 - val_acc: 0.8347\n",
            "Epoch 296/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4488 - acc: 0.8341 - val_loss: 0.4496 - val_acc: 0.8339\n",
            "Epoch 297/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4486 - acc: 0.8342 - val_loss: 0.4510 - val_acc: 0.8331\n",
            "Epoch 298/1000\n",
            "70000/70000 [==============================] - 63s 897us/step - loss: 0.4486 - acc: 0.8341 - val_loss: 0.4487 - val_acc: 0.8348\n",
            "Epoch 299/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4486 - acc: 0.8344 - val_loss: 0.4488 - val_acc: 0.8352\n",
            "Epoch 300/1000\n",
            "70000/70000 [==============================] - 63s 897us/step - loss: 0.4487 - acc: 0.8342 - val_loss: 0.4504 - val_acc: 0.8341\n",
            "Epoch 301/1000\n",
            "70000/70000 [==============================] - 63s 899us/step - loss: 0.4483 - acc: 0.8342 - val_loss: 0.4485 - val_acc: 0.8352\n",
            "Epoch 302/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4484 - acc: 0.8344 - val_loss: 0.4508 - val_acc: 0.8329\n",
            "Epoch 303/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4484 - acc: 0.8343 - val_loss: 0.4495 - val_acc: 0.8334\n",
            "Epoch 304/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4485 - acc: 0.8344 - val_loss: 0.4494 - val_acc: 0.8335\n",
            "Epoch 305/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4485 - acc: 0.8341 - val_loss: 0.4507 - val_acc: 0.8336\n",
            "Epoch 306/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4485 - acc: 0.8343 - val_loss: 0.4496 - val_acc: 0.8327\n",
            "Epoch 307/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4482 - acc: 0.8342 - val_loss: 0.4518 - val_acc: 0.8344\n",
            "Epoch 308/1000\n",
            "70000/70000 [==============================] - 64s 913us/step - loss: 0.4484 - acc: 0.8342 - val_loss: 0.4516 - val_acc: 0.8317\n",
            "Epoch 309/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4485 - acc: 0.8343 - val_loss: 0.4494 - val_acc: 0.8337\n",
            "Epoch 310/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4480 - acc: 0.8343 - val_loss: 0.4499 - val_acc: 0.8341\n",
            "Epoch 311/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4476 - acc: 0.8342 - val_loss: 0.4494 - val_acc: 0.8334\n",
            "Epoch 312/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4484 - acc: 0.8341 - val_loss: 0.4507 - val_acc: 0.8333\n",
            "Epoch 313/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4482 - acc: 0.8340 - val_loss: 0.4505 - val_acc: 0.8335\n",
            "Epoch 314/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4480 - acc: 0.8342 - val_loss: 0.4491 - val_acc: 0.8339\n",
            "Epoch 315/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4481 - acc: 0.8343 - val_loss: 0.4492 - val_acc: 0.8334\n",
            "Epoch 316/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4481 - acc: 0.8342 - val_loss: 0.4527 - val_acc: 0.8338\n",
            "Epoch 317/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4480 - acc: 0.8343 - val_loss: 0.4484 - val_acc: 0.8347\n",
            "Epoch 318/1000\n",
            "70000/70000 [==============================] - 63s 901us/step - loss: 0.4478 - acc: 0.8344 - val_loss: 0.4491 - val_acc: 0.8351\n",
            "Epoch 319/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4482 - acc: 0.8342 - val_loss: 0.4478 - val_acc: 0.8346\n",
            "Epoch 320/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4479 - acc: 0.8340 - val_loss: 0.4505 - val_acc: 0.8346\n",
            "Epoch 321/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4479 - acc: 0.8342 - val_loss: 0.4474 - val_acc: 0.8351\n",
            "Epoch 322/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4477 - acc: 0.8343 - val_loss: 0.4484 - val_acc: 0.8340\n",
            "Epoch 323/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4478 - acc: 0.8343 - val_loss: 0.4496 - val_acc: 0.8338\n",
            "Epoch 324/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4480 - acc: 0.8343 - val_loss: 0.4486 - val_acc: 0.8347\n",
            "Epoch 325/1000\n",
            "70000/70000 [==============================] - 63s 900us/step - loss: 0.4478 - acc: 0.8344 - val_loss: 0.4491 - val_acc: 0.8355\n",
            "Epoch 326/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4479 - acc: 0.8342 - val_loss: 0.4498 - val_acc: 0.8339\n",
            "Epoch 327/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4478 - acc: 0.8342 - val_loss: 0.4504 - val_acc: 0.8316\n",
            "Epoch 328/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4476 - acc: 0.8342 - val_loss: 0.4492 - val_acc: 0.8344\n",
            "Epoch 329/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4479 - acc: 0.8342 - val_loss: 0.4512 - val_acc: 0.8316\n",
            "Epoch 330/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4475 - acc: 0.8344 - val_loss: 0.4484 - val_acc: 0.8337\n",
            "Epoch 331/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4477 - acc: 0.8345 - val_loss: 0.4486 - val_acc: 0.8354\n",
            "Epoch 332/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4476 - acc: 0.8342 - val_loss: 0.4476 - val_acc: 0.8352\n",
            "Epoch 333/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4474 - acc: 0.8343 - val_loss: 0.4475 - val_acc: 0.8343\n",
            "Epoch 334/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4476 - acc: 0.8343 - val_loss: 0.4478 - val_acc: 0.8342\n",
            "Epoch 335/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4475 - acc: 0.8342 - val_loss: 0.4470 - val_acc: 0.8349\n",
            "Epoch 336/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4473 - acc: 0.8344 - val_loss: 0.4477 - val_acc: 0.8342\n",
            "Epoch 337/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4476 - acc: 0.8343 - val_loss: 0.4483 - val_acc: 0.8351\n",
            "Epoch 338/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4476 - acc: 0.8342 - val_loss: 0.4467 - val_acc: 0.8351\n",
            "Epoch 339/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4472 - acc: 0.8344 - val_loss: 0.4495 - val_acc: 0.8335\n",
            "Epoch 340/1000\n",
            "70000/70000 [==============================] - 63s 900us/step - loss: 0.4474 - acc: 0.8344 - val_loss: 0.4478 - val_acc: 0.8316\n",
            "Epoch 341/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4473 - acc: 0.8344 - val_loss: 0.4472 - val_acc: 0.8350\n",
            "Epoch 342/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4473 - acc: 0.8344 - val_loss: 0.4463 - val_acc: 0.8351\n",
            "Epoch 343/1000\n",
            "70000/70000 [==============================] - 64s 909us/step - loss: 0.4470 - acc: 0.8344 - val_loss: 0.4473 - val_acc: 0.8346\n",
            "Epoch 344/1000\n",
            "70000/70000 [==============================] - 63s 901us/step - loss: 0.4474 - acc: 0.8341 - val_loss: 0.4485 - val_acc: 0.8333\n",
            "Epoch 345/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4472 - acc: 0.8345 - val_loss: 0.4469 - val_acc: 0.8352\n",
            "Epoch 346/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4471 - acc: 0.8344 - val_loss: 0.4482 - val_acc: 0.8350\n",
            "Epoch 347/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4470 - acc: 0.8344 - val_loss: 0.4480 - val_acc: 0.8345\n",
            "Epoch 348/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4473 - acc: 0.8345 - val_loss: 0.4476 - val_acc: 0.8353\n",
            "Epoch 349/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4474 - acc: 0.8344 - val_loss: 0.4474 - val_acc: 0.8354\n",
            "Epoch 350/1000\n",
            "70000/70000 [==============================] - 63s 905us/step - loss: 0.4473 - acc: 0.8342 - val_loss: 0.4471 - val_acc: 0.8351\n",
            "Epoch 351/1000\n",
            "70000/70000 [==============================] - 64s 910us/step - loss: 0.4470 - acc: 0.8344 - val_loss: 0.4472 - val_acc: 0.8344\n",
            "Epoch 352/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4467 - acc: 0.8345 - val_loss: 0.4478 - val_acc: 0.8348\n",
            "Epoch 353/1000\n",
            "70000/70000 [==============================] - 64s 911us/step - loss: 0.4471 - acc: 0.8344 - val_loss: 0.4509 - val_acc: 0.8346\n",
            "Epoch 354/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4470 - acc: 0.8344 - val_loss: 0.4472 - val_acc: 0.8350\n",
            "Epoch 355/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4469 - acc: 0.8344 - val_loss: 0.4482 - val_acc: 0.8348\n",
            "Epoch 356/1000\n",
            "70000/70000 [==============================] - 63s 906us/step - loss: 0.4468 - acc: 0.8344 - val_loss: 0.4470 - val_acc: 0.8345\n",
            "Epoch 357/1000\n",
            "70000/70000 [==============================] - 63s 907us/step - loss: 0.4469 - acc: 0.8345 - val_loss: 0.4471 - val_acc: 0.8335\n",
            "Epoch 358/1000\n",
            "70000/70000 [==============================] - 63s 903us/step - loss: 0.4470 - acc: 0.8344 - val_loss: 0.4486 - val_acc: 0.8343\n",
            "Epoch 359/1000\n",
            "70000/70000 [==============================] - 63s 899us/step - loss: 0.4471 - acc: 0.8343 - val_loss: 0.4472 - val_acc: 0.8362\n",
            "Epoch 360/1000\n",
            "70000/70000 [==============================] - 63s 904us/step - loss: 0.4468 - acc: 0.8344 - val_loss: 0.4474 - val_acc: 0.8345\n",
            "Epoch 361/1000\n",
            "70000/70000 [==============================] - 63s 902us/step - loss: 0.4468 - acc: 0.8343 - val_loss: 0.4491 - val_acc: 0.8349\n",
            "Epoch 362/1000\n",
            "70000/70000 [==============================] - 64s 908us/step - loss: 0.4468 - acc: 0.8343 - val_loss: 0.4499 - val_acc: 0.8324\n",
            "Epoch 363/1000\n",
            "44672/70000 [==================>...........] - ETA: 20s - loss: 0.4466 - acc: 0.8344"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAUq_C3w7UHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ca0fdd1e-3166-4af8-f07b-441ae68bcefe"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7759714798e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447laAKZbZuL"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history2.history['acc'])\n",
        "plt.plot(history2.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blhNwzh7AAOp"
      },
      "source": [
        "single_prediction = model.predict(X[9:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyF_qiIvAALL"
      },
      "source": [
        "single_prediction = single_prediction.reshape(single_prediction.shape[1], single_prediction.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7t2ANVeCNSq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ccd26eea-07d8-41d6-dab9-4741c1469c9c"
      },
      "source": [
        "plt.imshow(single_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0996e595f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAD5CAYAAADrw7rWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezH-GfR5IoKH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "33ccdf48-5466-4a97-9a44-dcb36b7f2615"
      },
      "source": [
        "# plt.imshow(np.max(single_prediction))\n",
        "# np.argmax(single_prediction, axis=1)\n",
        "plt.imshow(to_categorical(np.argmax(single_prediction, axis=1), y_categorical.shape[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0996e33e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 475
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAD5CAYAAADrw7rWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqMx3gxc_gac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "cb661108-048b-4110-d5d7-097d3d7a8bae"
      },
      "source": [
        "plt.imshow(y_categorical[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0996d76940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 477
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAD5CAYAAADrw7rWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jIn9x5y__7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec8e5d0c-1fcc-4d8d-8c63-6010c1ef1f55"
      },
      "source": [
        "model.evaluate(X, y_categorical)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 1s 728us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.154971733093262, 0.47951724004745483]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeXCuoiwDslh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64820755-0197-4429-afa3-78fb611df9e7"
      },
      "source": [
        "y_categorical[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJEoybatldx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80519cb6-6d3f-45c0-b501-06efd93be784"
      },
      "source": [
        "gen = train_generator()\n",
        "sample = next(gen)\n",
        "sample[0].shape, sample[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10, 88, 1), (10, 88, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JS5wiAumRr"
      },
      "source": [
        "sample[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nF4w6La4V6x"
      },
      "source": [
        "sample[0][0,:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqbawlRd3Wnn"
      },
      "source": [
        "plt.imshow(sample[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0spGLFxQ4IWO"
      },
      "source": [
        "plt.imshow(sample[1][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gabZqjUc4K8M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}