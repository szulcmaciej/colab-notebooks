{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LunarLanderContinuous_TD3_Tensorboard.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szulcmaciej/colab-notebooks/blob/main/LunarLanderContinuous_TD3_Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "\n",
        "# **OpenAI Gym LunarLanderContinuous TD3 with Tensorboard & Video** \n",
        "\n",
        "This colab will allow you to train, evaluate and visulize your results using stable-baselines and tensorboard. Google colab don't support env.render() so we will use a work around where we \"fake\" a display, record a video and then display it. We will be using OpenAI Gym enviorment,  Stable-baselines & TD3\n",
        "\n",
        "## **Instructions**\n",
        "Click **Open in playground** top left corner.   \n",
        "Then either run cell by cell (recommended)   \n",
        "or just click \"Runtime\" in toolbar, then \"Run all\" leave the tab running,  \n",
        "check back in 30-60 min and scroll down top bottom\n",
        "\n",
        "### **Links** \n",
        "[https://stable-baselines.readthedocs.io/en/master/](https://stable-baselines.readthedocs.io/en/master/)  \n",
        "[https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93)\n",
        "\n",
        "----\n",
        "\n",
        "# **A Notebook from Nextgrid.ai**\n",
        "![Nextgrid Deep learning labs](https://nextgrid.ai/wp-content/uploads/2019/12/Deck-wallpaper-logo-scaled.jpg)\n",
        "\n",
        " \n",
        "### **Nextgrid** - _The **Superlative** destination for deep & reinforcement learning startups & talent_\n",
        "\n",
        "Learn more: [Deep learning labs](https://nextgrid.ai/deep-learning-labs/) / [Nextgrid](https://nextgrid.ai) \n",
        "\n",
        "\n",
        "\n",
        "▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️  \n",
        "*Notebook by Mathias*  \n",
        "*I would love your feedback,*  \n",
        "*or discuss your DL/DRL startup/business idea.*   \n",
        "*find me on* _[twitter](https://twitter.com/mathiiias123)_ or _[linkedin](https://www.linkedin.com/in/imathias)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22TBe2qeFlyr"
      },
      "source": [
        "## Install system wide packages\n",
        "Install linux server packages using `apt-get` and Python packages using `pip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq #remove -qq for full output\n",
        "!pip install stable-baselines==2.10.0 box2d box2d-kengz pyvirtualdisplay pyglet==1.5.0 --quiet #remove --quiet for full output \n",
        "# Remove all TensorBoard packages.\n",
        "! pip list --format=freeze | grep tensorboard | xargs pip uninstall -y\n",
        "# Install TensorFlow again (This command will only install the default TensorBoard package associated with this TensorFlow package). \n",
        "! pip install -q tensorflow\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Dependencis\n",
        "import dependencis required to run, train & record video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pToLfvOzCKQ"
      },
      "source": [
        "import gym\n",
        "import imageio\n",
        "import time\n",
        "import numpy as np\n",
        "import base64\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# Video \n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Stable baselines\n",
        "from stable_baselines import TD3\n",
        "from stable_baselines.td3.policies import MlpPolicy\n",
        "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlaTq_UvGKo-"
      },
      "source": [
        "# Define & Configure our Reinforcment learning algo\n",
        "Here we define our variables & Hyperparamters  \n",
        "In this example we are using default Twin Delayed DDPG.  \n",
        "Read more about how you define your TD3 [parameters](https://stable-baselines.readthedocs.io/en/master/modules/td3.html#parameters) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKA52SBe6JdJ"
      },
      "source": [
        "### Variables\n",
        "env_id = 'LunarLanderContinuous-v2'\n",
        "video_folder = '/videos'\n",
        "video_length = 3000\n",
        "logs_base_dir = './runs' # Log DIR\n",
        "steps_total= 0 # Keep track of total steps\n",
        "\n",
        "\n",
        "### Enviorment \n",
        "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "obs = env.reset()\n",
        "\n",
        "\n",
        "### Hyperparameters \n",
        "log_interval = 100          # Print avg reward after interval\n",
        "verbose = 1                 # Logging level\n",
        "gamma = 0.99                # Discount for future rewards\n",
        "learning_rate = 0.0003      # Learning rate\n",
        "buffer_size = 200000        # Size of the replay buffer\n",
        "batch_size = 256            # Batch_size: num of transitions sampled from replay buffer\n",
        "learning_starts = 1000        # Steps before starting training\n",
        "train_freq = 100            # Update the model every train_freq steps.\n",
        "gradient_steps = 50         # How many gradient update after each step\n",
        "# tau = 0.005\n",
        "policy_delay = 2            # Policy and target networks will only be updated once every policy_delay steps per training steps. The Q values will be updated policy_delay more often (update every training step).\n",
        "#target_policy_noise = 0.1  # Standard deviation of Gaussian noise added to target policy \n",
        "#target_noise_clip = 0.3    # Limit for absolute value of target policy smoothing noise.\n",
        "random_exploration = 0.1    # Probability of taking a random action\n",
        "#policy_kwargs = None\n",
        "#seed = None\n",
        "#n_cpu_tf_sess = 10\n",
        "#polyak = 0.995              # target policy update parameter (1-tau)\n",
        "#policy_noise = 0.2          # target policy smoothing noise\n",
        "#noise_clip = 0.5\n",
        "\n",
        "#### Action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "#### Model\n",
        "model = TD3(MlpPolicy, env, action_noise=action_noise, gamma=gamma, batch_size=batch_size,\n",
        "            learning_starts=learning_starts, buffer_size=buffer_size, learning_rate=learning_rate, random_exploration=random_exploration, \n",
        "            verbose=1,  tensorboard_log=logs_base_dir, train_freq=train_freq)\n",
        "\n",
        "# model = TD3(MlpPolicy, env, learning_rate=learning_rate, buffer_size=buffer_size, verbose=1,\n",
        "#             tensorboard_log=logs_base_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmK85N-4XdK7"
      },
      "source": [
        "## Training & Rec/Play Video [functions]\n",
        "\n",
        "- `def learning(name, steps=10000, prefix=env_id, eval=1000):`\n",
        "- `def record(name, length=1500):`  \n",
        "\n",
        "_that simply help us call the right functions to train our agent and to record & display video_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "# Training function\n",
        "def learning(name, steps=10000, prefix=env_id, eval=1000):\n",
        "  model.learn(total_timesteps=steps, log_interval=log_interval)\n",
        "  model.save(name + \"-\" + prefix)\n",
        "  # Random Agent, after training\n",
        "  # mean_reward_after_train = evaluate(model, num_steps=eval)\n",
        "\n",
        "\n",
        "def record(name, length=1500):\n",
        "   record_video(env_id, model, video_length=length, prefix=name)\n",
        "   show_videos('videos', prefix=name)\n",
        "   print(name, \" steps total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qcuGpFzhIZY"
      },
      "source": [
        "## Functions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-qEyqYl86uI"
      },
      "source": [
        "### Record & Display Video\n",
        "\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "# Record video\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()\n",
        "\n",
        "\n",
        "## Display video\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJQe53BM0ARa"
      },
      "source": [
        "# Display Tensorboard inline\n",
        "Run & Display tensorboard   \n",
        "**PS.** *sometimes it does not show up at all, then test to uncomment the reload code, or jusrt run cell again*\n",
        "\n",
        "It's correctly loaded when you see this view\n",
        "![Tensorboard](https://nextgrid.ai/wp-content/uploads/2019/12/Screenshot-2019-12-27-at-16.40.02.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMyX839yKVQB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5g4Xbc6HMs"
      },
      "source": [
        "# Often not loading on first try, run again until u see the screen\n",
        "%tensorboard --logdir {logs_base_dir}\n",
        "# %reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdCoU2aiKeII"
      },
      "source": [
        "### (Optional) Use Ngrok to display Tensorboard in another tab \n",
        "It's possible to display tensorboard in another tab, to do that simply cell and copy the printed url and past in to another tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaNU7cYAHxJ5"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "os.makedirs(logs_base_dir, exist_ok=True)\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(logs_base_dir))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PP-0aa90TNz"
      },
      "source": [
        "# Training Function\n",
        "We want to automate the training function so that it will keep running until the result we looking for is achived"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQ_Nt7d1NOP"
      },
      "source": [
        "\n",
        "steps_total = 0\n",
        "score = 0\n",
        "\n",
        "def run_training(steps_per_round=200000,limit=150):\n",
        "# This function will run a training with value set in `steps_per_round`\n",
        "# after each round it will messure it's value, If value is under `limit` it will keep training until score limit is reached.  \n",
        "\n",
        "  global score\n",
        "  global steps_total\n",
        "\n",
        "  print(\"Training is starting.. \")\n",
        "  \n",
        "  while score < limit:\n",
        "      steps_total = steps_total + steps_per_round\n",
        "      learning(str(steps_total), steps=steps_per_round)\n",
        "      new_evaluation = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
        "      score = new_evaluation[0]\n",
        "      record(name=steps_total, length=1000) # uncomment to show video from each round\n",
        "      print(\"Mean reward:\", score )\n",
        "    \n",
        "\n",
        "  # Threshold reached > evaluate over 100 episodes > Video rec/display\n",
        "  print(\"Reward limit achived, messuring over 100ep & recording video, please wait...\")\n",
        "  record(name=steps_total, length=1750)\n",
        "  ep100 = evaluate_policy(model, env, n_eval_episodes=50, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
        "  print(\"Mean Reward 100 Epispodes: \", ep100[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbbuMF60QmB"
      },
      "source": [
        "## Train moodel\n",
        "Add the amount of total moves that will be run before messuring results with `steps_per_round` parameter.  In `limnit` add the score you want model to reach to end training. If not reached it will simply run another round."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-imjiLk0Odz"
      },
      "source": [
        "# Traing\n",
        "run_training(steps_per_round=50000,limit=100) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecisEsrX0dKu"
      },
      "source": [
        "run_training(steps_per_round=100000,limit=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odLPx9ogu31Z"
      },
      "source": [
        "# Evaluation\n",
        "OpenAI scores is generally messured over 100 epochs. Use code belowe to messure your avarage score over 100 rounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7D2_Fjzf935"
      },
      "source": [
        "evals = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
        "print(evals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMR_hsEhk_bw"
      },
      "source": [
        "### Code demostrating how to save, delete & load model\n",
        "# model.save(\"save_as_name\")\n",
        "# del model # \n",
        "# model = TD3.load(\"name_of_model_to_load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfk5JmejMDoB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGsuEeAfUBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAVOQYwTeAF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}